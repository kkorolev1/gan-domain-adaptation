defaults:
  - models

name: "train"
n_gpu: 1

optimizer_encoder:
  _target_: torch.optim.Adam
  lr: 1e-4

lr_scheduler_encoder:
  _target_: torch.optim.lr_scheduler.ExponentialLR
  gamma: 0.9999

batch_size: 1

metrics:
  - _target_: degan.metric.MeanSemanticScore
    name: MeanSemanticScore
  - _target_: degan.metric.MeanDiversityScore
    name: MeanDiversityScore

loss:
  _target_: degan.loss.DELoss
  mult_direction: 50
  mult_domain_norm: 0.001
  mult_indomain_angle: 1

transform:
  _target_: torchvision.transforms.v2.Compose
  transforms:
    - _target_: torchvision.transforms.v2.Resize
      size: [1024, 1024]
    - _target_: torchvision.transforms.v2.ToImage
    - _target_: torchvision.transforms.v2.ToDtype
      dtype: 
        _target_: degan.utils.types.TorchFloat32
      scale: True
    - _target_: torchvision.transforms.v2.Normalize
      mean: [0.5, 0.5, 0.5]
      std: [0.5, 0.5, 0.5]

data:
  train:
    batch_size: ${batch_size}
    num_workers: 5
    datasets:
      - _target_: degan.datasets.DomainLatentDataset
        root_path: datasets/train
        domain_limit: 1
        merge_all: False
        transform: ${transform}
        sample_latent: True
  val:
    batch_size: 5
    num_workers: 5
    datasets:
      - _target_: degan.datasets.DomainLatentDataset
        root_path: datasets/train
        domain_limit: 1
        latent_limit: 15
        merge_all: True
        transform: ${transform}
        sample_latent: False

trainer: 
  epochs: 100
  save_dir: "saved/"
  save_period: 5
  verbosity: 2
  monitor: "min val_loss"
  early_stop: 100
  visualize: "wandb"
  wandb_project: "degan_project"
  wandb_run_name: "one_batch_test_new_encoder_bigger"
  len_epoch: 700
  grad_norm_clip: 100
  grad_accumulation_steps: 1
  mean_clip_emb: datasets/mean_clip_emb.pt

wandb_key: 91898ab676432e8d5689a2ce4a88f7131dc1e45c