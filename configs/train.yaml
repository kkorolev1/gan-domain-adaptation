defaults:
  - models

name: "train"
gpus: [1]

batch_size: 8
batch_expand_mult: 4
len_epoch: 200
mean_emb_path: datasets/mean_clip_emb.pt

optimizer_encoder:
  _target_: torch.optim.Adam
  lr: 1e-3

lr_scheduler_encoder:
  _target_: torch.optim.lr_scheduler.ExponentialLR
  gamma: 0.9999

# ema:
#   _target_: ema_pytorch.EMA
#   beta: 0.9999
#   update_after_step: 1
#   update_every: 50

metrics:
  - _target_: degan.metric.MeanSemanticScore
    iter_based: False
  - _target_: degan.metric.MeanDiversityScore
    iter_based: False

loss:
  _target_: degan.loss.CompositeLoss
  loss_modules:
    - _target_: degan.loss.DirectionLoss
      name: loss_direction
      mult: 1
    - _target_: degan.loss.TTDirectionLoss
      name: loss_tt_direction
      mult: 1
    - _target_: degan.loss.InDomainAngleLoss
      name: loss_indomain_angle
      mult: 12

transform:
  _target_: torchvision.transforms.v2.Compose
  transforms:
    - _target_: torchvision.transforms.v2.Resize
      size: [1024, 1024]
    - _target_: torchvision.transforms.v2.ToImage
    - _target_: torchvision.transforms.v2.ToDtype
      dtype: 
        _target_: degan.utils.types.TorchFloat32
      scale: True
    - _target_: torchvision.transforms.v2.Normalize
      mean: [0.5, 0.5, 0.5]
      std: [0.5, 0.5, 0.5]

data:
  train:
    batch_size: ${batch_size}
    num_workers: 8
    datasets:
      - _target_: degan.datasets.DomainLatentDataset
        root_path: datasets/train
        transform: ${transform}
        sample_latent: True
        batch_expand_mult: ${batch_expand_mult}
        mean_emb_path: ${mean_emb_path}
  val:
    batch_size: ${batch_size}
    num_workers: 8
    datasets:
      - _target_: degan.datasets.DomainLatentDataset
        root_path: datasets/val
        transform: ${transform}
        sample_latent: False
        mean_emb_path: ${mean_emb_path}
  # val_ema:
  #   batch_size: ${batch_size}
  #   num_workers: 5
  #   datasets:
  #     - _target_: degan.datasets.DomainLatentDataset
  #       root_path: datasets/val
  #       transform: ${transform}
  #       sample_latent: False

fade_in_scheduler:
  start: 0.1
  end: 1
  annealing_steps: 2000

trainer: 
  epochs: 200
  save_dir: "saved/"
  save_period: 5
  monitor: "max val_MeanSemanticScore"
  early_stop: 100
  wandb_project: "degan_project"
  wandb_run_name: "10k_domains_big_lr_decay_resume"
  len_epoch: ${len_epoch}
  grad_norm_clip: 100
  grad_accumulation_steps: 1

resume: /home/kirillkorolev/gan-domain-adaptation/saved/models/train/0515_010641/model_best.pth