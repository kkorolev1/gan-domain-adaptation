defaults:
  - models

name: "train"
gpus: [0]

batch_size: 4
len_epoch: 1000

optimizer_encoder:
  _target_: torch.optim.Adam
  lr: 1e-4

lr_scheduler_encoder:
  _target_: torch.optim.lr_scheduler.ExponentialLR
  gamma: 1

# ema:
#   _target_: ema_pytorch.EMA
#   beta: 0.9999
#   update_after_step: 1
#   update_every: 50

metrics:
  - _target_: degan.metric.MeanSemanticScore
    name: MeanSemanticScore
  - _target_: degan.metric.MeanDiversityScore
    name: MeanDiversityScore

loss:
  _target_: degan.loss.CompositeLoss
  loss_modules:
    - _target_: degan.loss.DirectionLoss
      name: loss_direction
      mult: 10
    - _target_: degan.loss.TTDirectionLoss
      name: loss_tt_direction
      mult: 0.4
    - _target_: degan.loss.DomainNormLoss
      name: loss_domain_norm
      mult: 0.001

transform:
  _target_: torchvision.transforms.v2.Compose
  transforms:
    - _target_: torchvision.transforms.v2.Resize
      size: [1024, 1024]
    - _target_: torchvision.transforms.v2.ToImage
    - _target_: torchvision.transforms.v2.ToDtype
      dtype: 
        _target_: degan.utils.types.TorchFloat32
      scale: True
    - _target_: torchvision.transforms.v2.Normalize
      mean: [0.5, 0.5, 0.5]
      std: [0.5, 0.5, 0.5]

data:
  train:
    batch_size: ${batch_size}
    num_workers: 5
    datasets:
      - _target_: degan.datasets.DomainLatentDataset
        root_path: datasets/train
        domain_limit: 10000
        transform: ${transform}
        sample_latent: True
  val:
    batch_size: ${batch_size}
    num_workers: 5
    datasets:
      - _target_: degan.datasets.DomainLatentDataset
        root_path: datasets/val
        transform: ${transform}
        sample_latent: False
  # val_ema:
  #   batch_size: ${batch_size}
  #   num_workers: 5
  #   datasets:
  #     - _target_: degan.datasets.DomainLatentDataset
  #       root_path: datasets/val
  #       transform: ${transform}
  #       sample_latent: False

trainer: 
  epochs: 100
  save_dir: "saved/"
  save_period: 5
  verbosity: 2
  monitor: "max val_MeanSemanticScore"
  early_stop: 100
  visualize: "wandb"
  wandb_project: "degan_project"
  wandb_run_name: "ViT_tt_loss_small_bs"
  len_epoch: ${len_epoch}
  grad_norm_clip: 100
  grad_accumulation_steps: 2

wandb_key: 91898ab676432e8d5689a2ce4a88f7131dc1e45c